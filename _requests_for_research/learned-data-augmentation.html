<p>⭐⭐⭐ <strong>Learned Data Augmentation.</strong> You could use a learned <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" target="_blank" rel="noopener">VAE</a> of data, to perform “learned data augmentation”. One would first train a VAE on input data, then each training point would be transformed by encoding to a latent space, then applying a simple (e.g. Gaussian) perturbation in latent space, then decoding back to observed space. Could we use such an approach to obtain improved generalization? A potential benefit of such data augmentation is that it could include many nonlinear transformations like viewpoint changes and changes in scene lightning. Can we approximate the set of transformations to which the label is invariant? Check out the <a href="https://arxiv.org/abs/1611.01331" target="_blank" rel="noopener">existing</a> <a href="https://arxiv.org/abs/1702.05538" target="_blank" rel="noopener">work</a> <a href="https://arxiv.org/abs/1709.01643" target="_blank" rel="noopener">on</a> <a href="https://arxiv.org/abs/1711.04340" target="_blank" rel="noopener">this</a> <a href="https://arxiv.org/abs/1711.00648" target="_blank" rel="noopener">topic</a> <a href="http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf" target="_blank" rel="noopener">if</a> <a href="https://arxiv.org/abs/1710.10564" target="_blank" rel="noopener">you</a> <a href="https://papers.nips.cc/paper/7278-learning-to-model-the-tail" target="_blank" rel="noopener">want</a> a place to get&nbsp;started.</p>

<h3>Solutions</h3>
<p> Preliminary paper describing learned data augmentation can be found <a href="https://github.com/chamorajg/learned-data-augmentation/blob/main/procedure/learned_data_augmentation.pdf"> here </a>, with the implementation avaiable <a href="https://github.com/chamorajg/learned-data-augmentation"> at this repo </a>. Current results improve the generalization of the VAE when tested on the CIFAR-10 dataset.</p>